"""
LM Studio + RAG Integration Module
----------------------------------

Provides a lightweight wrapper for interacting with a local LLM server via LM Studio. 
Includes utilities for document ingestion via Chroma, embedding generation with GPT4AllEmbeddings,
and construction of retrieval-augmented prompts.

This module is designed to assist in risk reporting and interpretability by combining
portfolio metrics with contextual information retrieved from PDF knowledge bases.

Usage Instructions
------------------
1. Download LM Studio from https://lmstudio.ai/
2. In LM Studio:
   - Go to "Discover", install a model (we use `qwen-3-4b-instruct`)
   - Go to "Developer", load the model and click "Start Server"
3. Set `LMSTUDIO_ENDPOINT` to the HTTP endpoint shown in the Developer tab.

Authors
-------
Niccolò Lecce, Alessandro Dodon, Marco Gasparetti

Created
-------
May 2025

Contents
--------
- ask_llm: Send a prompt to the local model and get a response
- get_vectorstore: Create or load a Chroma vector DB from one or more PDFs
- build_rag_prompt: Combine PDF context and portfolio metrics into a structured prompt
"""


#----------------------------------------------------------
# Packages
#----------------------------------------------------------
import requests
from langchain_chroma import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import GPT4AllEmbeddings
import os


# --- CONFIGURATION --------------------------
LMSTUDIO_ENDPOINT = "http://xxx.xxx.x.xxx:xxxx"  # Your LM Studio server URL, you can find this on the right side of the LM Studio Developer tab
API_PATH = "/v1/completions"
MODEL_NAME = "qwen-3-4b-instruct"                # Installed model name
# ---------------------------------------------


#----------------------------------------------------------
# Prompt Function for LLM
#----------------------------------------------------------
def ask_llm(prompt: str,
            max_tokens: int = 256,
            temperature: float = 0.2):
    """
    Main
    ----
    Sends a prompt to the locally running LM Studio LLM and retrieves the model's response.
    Uses the model via HTTP POST.

    Parameters
    ----------
    prompt : str
        The textual prompt to send to the model.
    max_tokens : int, optional
        Maximum number of tokens to generate. Default is 256.
    temperature : float, optional
        Sampling temperature (0 = deterministic, >0 = more random). Default is 0.2.

    Returns
    -------
    str
        Text response generated by the model.

    Raises
    ------
    HTTPError
        If the local LM Studio server fails to respond or returns an error.
    """
    url = LMSTUDIO_ENDPOINT.rstrip("/") + API_PATH
    payload = {
        "model": MODEL_NAME,
        "prompt": prompt,
        "max_tokens": max_tokens,
        "temperature": temperature
    }
    resp = requests.post(url, json=payload, timeout=400)  # 30 seconds timeout
    resp.raise_for_status()
    data = resp.json()
    return data["choices"][0]["text"]


#----------------------------------------------------------
# RAG Function (Chroma vector from a PDF)
#----------------------------------------------------------
def get_vectorstore(pdf_paths, persist_dir="kb_chroma"):
    """
    Main
    ----
    Builds or loads a persistent Chroma vector database from one or more PDF documents.
    If the directory exists, loads the existing vector index. Otherwise, processes the PDFs
    to generate a new vectorstore using GPT4All CPU embeddings.

    Parameters
    ----------
    pdf_paths : str or list of str
        Path(s) to one or more PDF files to be indexed.
    persist_dir : str, optional
        Directory where the Chroma index is stored. Default is "kb_chroma".

    Returns
    -------
    Chroma
        A Chroma vector database object ready for semantic search.
    """
    embedding = GPT4AllEmbeddings(device="cpu")  # CPU-based embedding model

    if os.path.isdir(persist_dir):
        # 1) Load existing Chroma index
        vectordb = Chroma(
            persist_directory=persist_dir,
            collection_name="kb_pdf",
            embedding_function=embedding
        )
    else:
        # 2) Build a new vectorstore from PDF
        loader = PyPDFLoader(pdf_paths)                    # Load PDF document
        docs = loader.load()
        splitter = RecursiveCharacterTextSplitter(         # Split into overlapping chunks
            chunk_size=500, chunk_overlap=100
        )
        chunks = splitter.split_documents(docs)
        vectordb = Chroma.from_documents(                  # Create and persist vectorstore
            documents=chunks,
            embedding=embedding,
            persist_directory=persist_dir,
            collection_name="kb_pdf"
        )
    return vectordb


#----------------------------------------------------------
# Prompt Generation (based on documents + metrics)
#----------------------------------------------------------
def build_rag_prompt(metrics: dict,
                     vectordb: Chroma,
                     port_val: float,
                     base: str,
                     k: int = 4) -> str:
    """
    Main
    ----
    Constructs a rich prompt for the local LLM by integrating:
    - Semantic search results from a PDF-based Chroma vectorstore
    - Portfolio risk metrics
    - Portfolio valuation context

    Designed to support client-facing explanations of portfolio risk metrics.

    Parameters
    ----------
    metrics : dict
        Dictionary of computed portfolio risk metrics (e.g., VaR, ES).
    vectordb : Chroma
        A Chroma vector database loaded with financial documentation.
    port_val : float
        Total monetary value of the portfolio.
    base : str
        Base currency of the portfolio (e.g., 'CHF', 'USD').
    k : int, optional
        Number of document chunks to retrieve via similarity search. Default is 4.

    Returns
    -------
    str
        Fully constructed prompt to be passed to the LLM for generation.
    """
    # Create a query based on the metrics dictionary
    query = ("Use the following documents to interpret these risk metrics:\n"
             + str(metrics))
    hits = vectordb.similarity_search(query, k=k)          # Retrieve top-k relevant chunks
    context = "\n\n".join(doc.page_content for doc in hits)

    # Final prompt passed to the LLM
    prompt = f"""
    You are a Senior Financial Risk Analyst with deep expertise in quantitative risk models and practical portfolio management. 
    You will be given: 

    1. **Supporting documentation excerpts** (do NOT quote verbatim; instead, internalize the concepts and rephrase them).
    2. **Computed portfolio risk metrics** (VaR, ES, backtest violation counts, rates, p-values for Kupiec, Christoffersen and Joint tests).
    3. **Total portfolio value** in the base currency.

    Your task is to produce a polished, non-technical briefing for a client, in clear English, organized exactly as follows:

    ---
    ## 1. Context Summary  
    — In 2–3 sentences, summarize the key ideas from the provided documentation that are relevant to understanding these risk metrics.

    ## 2. Metric Definitions  
    For each of the following risk metrics and tests:
    - Historical VaR  
    - Parametric (Normal) VaR  
    - EVT VaR & ES  
    - GARCH(1,1) VaR  
    - Any other models you see in the metrics  
    - Kupiec, Christoffersen, and Joint backtest statistics

    provide:
    1. **Definition**: one concise sentence stating what it measures and why it matters.  
    2. **Ideal Use Cases**: 2–3 bullet points describing when this metric or test is most useful in practice.  
    3. **Pros**: at least 3 bullet points highlighting the strengths.  
    4. **Cons**: at least 3 bullet points highlighting the limitations or pitfalls.  
    5. **Interpretation Guidance**: specifically for backtesting metrics, explain what the observed violation count, violation rate, and p-values imply about model reliability.  

    ## 3. Portfolio Summary  
    — State the total portfolio value (`{port_val:,.2f} {base}`) and explain in one sentence how that scales your VaR/ES numbers into monetary terms.

    ## 4. Best‐Practice Recommendations  
    — Provide 5 actionable, practical tips for improving portfolio risk management, drawn from both the models and the documentation. Use non-technical language and direct advice (e.g., “Consider adding a volatility overlay when…”).

    ## 5. Executive Summary  
    — Write a 2–3 sentence high-level take-away that a non-expert C-suite executive could read in under 30 seconds and still grasp the portfolio’s risk posture.

    **Format requirements**  
    - Use Markdown headings (##, ###) exactly as above.  
    - Use complete sentences.  
    - Keep each bullet point under 20 words.  
    - Do not quote the documentation verbatim—always paraphrase.  
    - Be precise, concise, and jargon-free.  
    """
    return prompt.strip()
