"""
LM Studio + RAG Integration Module
----------------------------------

Provides a lightweight wrapper for interacting with a local LLM server via LM Studio. 
Includes utilities for document ingestion via Chroma, embedding generation with GPT4AllEmbeddings,
and construction of retrieval-augmented prompts.

This module is designed to assist in risk reporting and interpretability by combining
portfolio metrics with contextual information retrieved from PDF knowledge bases.

Usage Instructions
------------------
1. Download LM Studio from https://lmstudio.ai/
2. In LM Studio:
   - Go to "Discover", install a model (we use `qwen-3-4b-instruct`)
   - Go to "Developer", load the model and click "Start Server"
3. Set `LMSTUDIO_ENDPOINT` to the HTTP endpoint shown in the Developer tab.

Authors
-------
Niccolò Lecce, Alessandro Dodon, Marco Gasparetti

Created
-------
May 2025

Contents
--------
- ask_llm: Send a prompt to the local model and get a response
- get_vectorstore: Create or load a Chroma vector DB from one or more PDFs
- build_rag_prompt: Combine PDF context and portfolio metrics into a structured prompt
"""


#----------------------------------------------------------
# Packages
#----------------------------------------------------------
import requests
from langchain_chroma import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import GPT4AllEmbeddings
import os


# --- CONFIGURATION --------------------------
LMSTUDIO_ENDPOINT = "http://xxx.xxx.x.xxx:xxxx"  # Your LM Studio server URL, you can find this on the right side of the LM Studio Developer tab
API_PATH = "/v1/completions"
MODEL_NAME = "qwen-3-4b-instruct"                # Installed model name
# ---------------------------------------------


#----------------------------------------------------------
# Prompt Function for LLM
#----------------------------------------------------------
def ask_llm(prompt: str,
            max_tokens: int = 256,
            temperature: float = 0.2):
    """
    Main
    ----
    Sends a prompt to the locally running LM Studio LLM and retrieves the model's response.
    Uses the model via HTTP POST.

    Parameters
    ----------
    prompt : str
        The textual prompt to send to the model.
    max_tokens : int, optional
        Maximum number of tokens to generate. Default is 256.
    temperature : float, optional
        Sampling temperature (0 = deterministic, >0 = more random). Default is 0.2.

    Returns
    -------
    str
        Text response generated by the model.

    Raises
    ------
    HTTPError
        If the local LM Studio server fails to respond or returns an error.
    """
    url = LMSTUDIO_ENDPOINT.rstrip("/") + API_PATH
    payload = {
        "model": MODEL_NAME,
        "prompt": prompt,
        "max_tokens": max_tokens,
        "temperature": temperature
    }
    resp = requests.post(url, json=payload, timeout=60)
    resp.raise_for_status()
    data = resp.json()
    return data["choices"][0]["text"]


#----------------------------------------------------------
# RAG Function (Chroma vector from a PDF)
#----------------------------------------------------------
def get_vectorstore(pdf_paths, persist_dir="kb_chroma"):
    """
    Main
    ----
    Builds or loads a persistent Chroma vector database from one or more PDF documents.
    If the directory exists, loads the existing vector index. Otherwise, processes the PDFs
    to generate a new vectorstore using GPT4All CPU embeddings.

    Parameters
    ----------
    pdf_paths : str or list of str
        Path(s) to one or more PDF files to be indexed.
    persist_dir : str, optional
        Directory where the Chroma index is stored. Default is "kb_chroma".

    Returns
    -------
    Chroma
        A Chroma vector database object ready for semantic search.
    """
    embedding = GPT4AllEmbeddings(device="cpu")  # CPU-based embedding model

    if os.path.isdir(persist_dir):
        # 1) Load existing Chroma index
        vectordb = Chroma(
            persist_directory=persist_dir,
            collection_name="kb_pdf",
            embedding_function=embedding
        )
    else:
        # 2) Build a new vectorstore from PDF
        loader = PyPDFLoader(pdf_paths)                    # Load PDF document
        docs = loader.load()
        splitter = RecursiveCharacterTextSplitter(         # Split into overlapping chunks
            chunk_size=500, chunk_overlap=100
        )
        chunks = splitter.split_documents(docs)
        vectordb = Chroma.from_documents(                  # Create and persist vectorstore
            documents=chunks,
            embedding=embedding,
            persist_directory=persist_dir,
            collection_name="kb_pdf"
        )
    return vectordb


#----------------------------------------------------------
# Prompt Generation (based on documents + metrics)
#----------------------------------------------------------
def build_rag_prompt(metrics: dict,
                     vectordb: Chroma,
                     port_val: float,
                     base: str,
                     k: int = 4) -> str:
    """
    Main
    ----
    Constructs a rich prompt for the local LLM by integrating:
    - Semantic search results from a PDF-based Chroma vectorstore
    - Portfolio risk metrics
    - Portfolio valuation context

    Designed to support client-facing explanations of portfolio risk metrics.

    Parameters
    ----------
    metrics : dict
        Dictionary of computed portfolio risk metrics (e.g., VaR, ES).
    vectordb : Chroma
        A Chroma vector database loaded with financial documentation.
    port_val : float
        Total monetary value of the portfolio.
    base : str
        Base currency of the portfolio (e.g., 'CHF', 'USD').
    k : int, optional
        Number of document chunks to retrieve via similarity search. Default is 4.

    Returns
    -------
    str
        Fully constructed prompt to be passed to the LLM for generation.
    """
    # Create a query based on the metrics dictionary
    query = ("Use the following documents to interpret these risk metrics:\n"
             + str(metrics))
    hits = vectordb.similarity_search(query, k=k)          # Retrieve top-k relevant chunks
    context = "\n\n".join(doc.page_content for doc in hits)

    # Final prompt passed to the LLM
    prompt = f"""
    You are a Senior Financial Risk Analyst. Use the following supporting
    documentation (do NOT quote verbatim; just internalize the ideas):

    {context}

    Here are the portfolio risk metrics:
    {metrics}

    The portfolio value is {port_val:,.2f} {base}.

    For each metric give:
    1. A brief definition.
    2. Ideal use cases.
    3. Pros (≥3 bullets).
    4. Cons (≥3 bullets).
    5. Best-practice hints for risk management.

    Explain in plain language suitable for a non-expert client.
    """
    return prompt.strip()
